{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c458c47dfa32f5d11deeed3666c6f69a153f3a21"
   },
   "source": [
    "# Introdução\n",
    "\n",
    "Neste kernel usaremos python para programar um agente de *reinforcement learning* para solucionar o jogo de *black jack*, famoso \"vinte e um\". Nosso jogador será treinado em um ambiente simulado e aprenderá sozinho a vencer a banca (ou pelo menos tentar).\n",
    "\n",
    "Esta aula é baseada no micro-desafio relacionado do [Kaggle](https://www.kaggle.com/fernandocanteruccio/blackjack-microchallenge-reinforce-agent).\n",
    "\n",
    "![Blackjack](http://www.hightechgambling.com/sites/default/files/styles/large/public/casino/table_games/blackjack.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Regras do Blackjack\n",
    "\n",
    "Usaremos uma versão ligeiramente simplificada do blackjack (também conhecido como vinte e um). Nesta versão, há um jogador e um dealer. O jogo prossegue da seguinte forma:\n",
    "\n",
    "- O jogador recebe duas cartas viradas para cima. O dealer recebe uma carta com a face para cima.\n",
    "- O jogador pode pedir para receber outra carta ('pedir') quantas vezes quiser. Se a soma de suas cartas exceder 21, ele perde a rodada imediatamente.\n",
    "- O *dealer* então distribui cartas adicionais para si mesmo até:\n",
    "    - A soma das cartas do *dealer* exceder 21, nesse caso o jogador vence a rodada, ou\n",
    "    - A soma das cartas do *dealer* ser maior ou igual a 17. Se o total do jogador for maior que o do *dealer*, o jogador vence. Caso contrário, o *dealer* vence (mesmo em caso de empate).\n",
    "\n",
    "Ao calcular a soma de cartas, Valete, Dama e Rei contam como 10. Ases podem contar como 1 ou 11. (Quando se refere ao \"total\" de um jogador acima, queremos dizer o maior total que pode ser feito sem exceder 21. A + 8 = 19, A + 8 + 8 = 17.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "324b624d17b3a51263115128e5dd87e6be621198"
   },
   "source": [
    "# A mesa\n",
    "\n",
    "Certo, com o conhecimento das regras do jogo, podemos escrever um algoritmo básico para simular uma 'mesa'.\n",
    "\n",
    "Vamos começar importando nossas dependências, neste caso, apenas o [Numpy](https://numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1ed70efd9e03b72c335511ffb39254cc4225b9cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9825893bcf078af81c583f78d7777c0fb9afb01"
   },
   "source": [
    "Para jogar cartas precisamos de um baralho, certo? Vamos implementar um!\n",
    "\n",
    "Utilizaremos um gerador para simular nosso baralho, assim poderemos tirar uma carta por vez mantendo um estado que nos indica quais cartas já foram utilizadas. Nosso baralho é descrito por uma lista de números de 1 a 11, sendo que teremos quatro 10 de cada nipe, representando o número 10 em si, além do Valete, da Rainha e do Rei.\n",
    "\n",
    "Embaralhamos o baralho com a função shuffle antes de realizar a primeira compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator for card drawing without repetition\n",
    "def get_deck():\n",
    "    cards = (list(range(1, 9)) + [10] * 4 + [11]) * 4\n",
    "    shuffle(cards)\n",
    "    while cards:\n",
    "        yield cards.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já podemos instanciar um baralho e utiliza-lo para comprar cartas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "deck = get_deck()\n",
    "for i in range(2):\n",
    "    print(next(deck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Vinte e um tem regras bem específicas. Precisamos calcular o valor total de uma mão dependendo do número de Ases presentes na mesma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total(aces, partial):\n",
    "    total = partial\n",
    "    for i in range(aces):\n",
    "        if partial + 11 > 21:\n",
    "            total += 1          # if score > 21, aces have value 1\n",
    "        else:\n",
    "            total += 11         # otherwise, aces have value 11\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A representação de nossa mesa consiste no valor total e no número de Ases na mão de cada jogador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment(dealer_partial, dealer_aces, player_partial, player_aces):\n",
    "    return (calculate_total(player_aces, player_partial),\n",
    "        calculate_total(dealer_aces, dealer_partial),\n",
    "        player_aces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo as regras descritas, podemos simular um jogo completo. Utilizamos variáveis de estado para manter os valores parciais e o número de Ases na mão de cada jogador. A cada rodada, atualizamos os valores destas variáveis para refletir as cartas sendo compradas.\n",
    "\n",
    "No fim do jogo, retornamos 1 caso o jogador vença e 0 caso o jogador \"estoure\" o limite ou o *dealer* vença por pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_game(agent):\n",
    "    # init scores and deck\n",
    "    dealer_partial = 0\n",
    "    dealer_aces = 0\n",
    "    player_partial = 0\n",
    "    player_aces = 0\n",
    "    deck = get_deck()\n",
    "    \n",
    "    # initial draw for the player\n",
    "    for _ in range(2):\n",
    "        card = next(deck)\n",
    "        if card == 11:\n",
    "            player_aces += 1\n",
    "        else:\n",
    "            player_partial += card\n",
    "            \n",
    "    # then for the dealer\n",
    "    card = next(deck)\n",
    "    if card == 11:\n",
    "        dealer_aces += 1\n",
    "    else:\n",
    "        dealer_partial += card\n",
    "    \n",
    "    # player's turn\n",
    "    # draw cards according to the provided policy\n",
    "    while agent(*get_environment(dealer_partial, dealer_aces, player_partial, player_aces)):\n",
    "        card = next(deck)\n",
    "        if card == 11:\n",
    "            player_aces += 1\n",
    "            \n",
    "        else:\n",
    "            player_partial += card\n",
    "            \n",
    "        if calculate_total(player_aces, player_partial) > 21:\n",
    "            return 0 # return 0 indicating house's victory\n",
    "        \n",
    "    # dealer's turn\n",
    "    while calculate_total(dealer_aces, dealer_partial) < 17:\n",
    "        card = next(deck)\n",
    "        if card == 11:\n",
    "            dealer_aces += 1\n",
    "        else:\n",
    "            dealer_partial += card\n",
    "            \n",
    "    # calculate totals\n",
    "    player_total = calculate_total(player_aces, player_partial)\n",
    "    dealer_total = calculate_total(dealer_aces, dealer_partial)\n",
    "    \n",
    "    # return 1 for player's victory, 0 otherwise\n",
    "    if dealer_total > 21 or player_total > dealer_total:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simular vários jogos e nos dar estatísticas da qualidade do nosso jogador, utilizamos um loop simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(agent, n_games=5):\n",
    "    player_victories = 0\n",
    "    for i in range(n_games):\n",
    "        player_victories += simulate_game(agent)\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f'{i}/{n_games} - Player won {player_victories} out of {i} (win rate = {player_victories / (i + 1) * 100}%)', end='\\r')\n",
    "        \n",
    "    print(f'\\nPlayer won {player_victories} out of {n_games} (win rate = {player_victories / n_games * 100}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O jogador de blackjack\n",
    "\n",
    "Nosso jogador será representado pela classe *Agent*. A aplicação da instância deverá corresponder a estratégia do jogador dado a mesa atual.\n",
    "\n",
    "Vamos implementar nossa classe base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __call__(self, *state):\n",
    "        raise NotImplementedError('You need to overwrite the __call__ method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certo, já podemos testar nossa simulação com algumas estratégias. Vamos começar simulando um perdedor, um jogador que sempre \"estoura\" os 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Looser(Agent):\n",
    "    def __call__(self, *state):\n",
    "        \"\"\"This should never win, for sanity check\"\"\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como um teste de validade da nossa função de simulação, vamos perder mil jogos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good till here.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    assert not simulate_game(Looser())\n",
    "    \n",
    "print('All good till here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma estratégia simples é sempre pedir até completar 17 pontos. Vamos tentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(Agent):\n",
    "    def __call__(self, *state):\n",
    "        \"\"\"Always call until 17, stop otherwise\"\"\"\n",
    "        if state[0] < 17:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/5000 - Player won 402 out of 999 (win rate = 40.2%)\r",
      "1999/5000 - Player won 812 out of 1999 (win rate = 40.6%)\r",
      "2999/5000 - Player won 1226 out of 2999 (win rate = 40.86666666666667%)\r",
      "3999/5000 - Player won 1661 out of 3999 (win rate = 41.525%)\r",
      "4999/5000 - Player won 2055 out of 4999 (win rate = 41.099999999999994%)\r\n",
      "Player won 2055 out of 5000 (win rate = 41.099999999999994%)\n"
     ]
    }
   ],
   "source": [
    "simulate(Simple(), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42% pode não parecer muito, mas essa estratégia é muito próximo do ótimo para este jogo.\n",
    "\n",
    "## Aprendizado de Máquina\n",
    "\n",
    "Vamos ver o que podemos fazer utilizando técnicas de *reinforcement learning*.\n",
    "\n",
    "Por se tratar de um jogo muito simples, podemos utilizar uma técnica também simples. Vamos implementar um modelo linear nos fatores do ambiente e treinaremos esse modelo utilizando o algoritmo *REINFORCE Policy Gradients*, de Willians.\n",
    "\n",
    "O pseudo código para esse algoritmo é o seguinte:\n",
    "\n",
    "```\n",
    "função REINFORCE\n",
    "    inicialize theta arbitrário\n",
    "    para cada episodio {s1, a1, r2, ..., st-1, at-1, rt} ~ pi_theta faça:\n",
    "        para t = 1 até T - 1 faça\n",
    "            theta <- theta + alpha.delta_theta.log_pi_theta(st, at).vt\n",
    "        fim for\n",
    "    fim for\n",
    "    returne theta\n",
    "fim função\n",
    "```\n",
    "\n",
    "Vamos implementar uma subclasse de nosso agente original para utilizar em nosso ambiente simulado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(Agent):\n",
    "    '''\n",
    "    REINFORCE Policy Gradients agent with softmax shallow model\n",
    "    https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf\n",
    "    https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n",
    "    https://medium.com/samkirkiles/reinforce-policy-gradients-from-scratch-in-numpy-6a09ae0dfe12\n",
    "    '''\n",
    "    def __init__(self, state_dim, n_actions, learning_rate, gamma, train=False):\n",
    "        ''' Holds our model weights and hyper parameters '''\n",
    "        self._train = train\n",
    "        self.w = np.random.rand(state_dim, n_actions) * 0.1\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.g = gamma\n",
    "        self.grads = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def __call__(self, *state):\n",
    "        if self._train:\n",
    "            return self.train(*state)\n",
    "        else:\n",
    "            return self.predict(*state)\n",
    "            \n",
    "    @staticmethod\n",
    "    def preprocess_state(state):\n",
    "        ''' Center and scale environment variables '''\n",
    "        return np.array([\n",
    "            state[0] / 21 - 0.5,\n",
    "            state[1] / 21 - 0.5,\n",
    "            state[2] / 4 - 0.5\n",
    "        ]).reshape((1, -1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_grad(softmax):\n",
    "        ''' Vectorized softmax Jacobian '''\n",
    "        s = softmax.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "    \n",
    "    def policy(self, state):\n",
    "        ''' Our policy that maps state to action parameterized by w '''\n",
    "        exp = np.exp(state.dot(self.w))\n",
    "        probs = exp / np.sum(exp)\n",
    "        action = np.random.choice(self.n_actions, p=probs[0])\n",
    "        return action, probs\n",
    "    \n",
    "    def predict(self, *state):\n",
    "        '''\n",
    "        Perform model inference and select\n",
    "        the best action greedly\n",
    "        '''\n",
    "        state = self.preprocess_state(state)\n",
    "        return np.argmax(self.policy(state)[1][0])\n",
    "        \n",
    "    def train(self, *state):\n",
    "        '''\n",
    "        Perform model inference, select action\n",
    "        and store gradients for training\n",
    "        '''\n",
    "        state = self.preprocess_state(state)\n",
    "        action, probs = self.policy(state)\n",
    "        dsoftmax = self.softmax_grad(probs)[action,:]\n",
    "        dlog = dsoftmax / probs[0, action]\n",
    "        grad = state.T.dot(dlog[None,:])\n",
    "        self.grads.append(grad)\n",
    "        self.rewards.append(0)\n",
    "        return action\n",
    "\n",
    "    def update(self, reward):\n",
    "        '''\n",
    "        Use the returned reward to compute gradients\n",
    "        and update model parameters\n",
    "        '''\n",
    "        self.rewards[-1] = reward\n",
    "    \n",
    "        for i in range(len(self.grads)):\n",
    "            # Loop through everything that happend in the episode and update towards the log policy gradient times **FUTURE** reward\n",
    "            self.w += self.lr * self.grads[i] * sum([r * (self.g ** r) for t, r in enumerate(self.rewards[i:])])\n",
    "        \n",
    "        self.grads = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda temos que escrever uma rotina de treinamento para o nosso modelo.\n",
    "\n",
    "Simularemos o resultado de vários jogos e, para cada jogo, atualizaremos nosso modelo com base no resultado de suas ações. Queremos utilizar como sinal de treinamento valores entre -1 e 1, sendo -1 reforço negativo e 1 reforço positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, n_games=5):\n",
    "    player_victories = 0\n",
    "    for i in range(n_games):\n",
    "        reward = simulate_game(agent)\n",
    "        player_victories += reward\n",
    "        agent.update(reward * 2 - 1)\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f'{i}/{n_games} - Player won {player_victories} out of {i} (win rate = {player_victories / (i + 1) * 100}%)', end='\\r')\n",
    "        \n",
    "    print(f'\\nPlayer won {player_victories} out of {n_games} (win rate = {player_victories / n_games * 100}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player won 202905 out of 500000 (win rate = 40.581%) rate = 40.581%)595190381%))\n"
     ]
    }
   ],
   "source": [
    "reinforce = REINFORCE(3, 2, 0.01, 0.999, train=True)\n",
    "train(reinforce, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o agente já treinado, podemos avaliar seu desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999/50000 - Player won 20799 out of 49999 (win rate = 41.598%)5306122446%)\n",
      "Player won 20799 out of 50000 (win rate = 41.598%)\n"
     ]
    }
   ],
   "source": [
    "reinforce._train = False\n",
    "simulate(reinforce, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo aprende!\n",
    "\n",
    "O resultado ficou dentro do esperado, em aproximadamente 42% vitórias. Esse jogo é desenhado para ser vantajoso para a banca, não importa o quão inteligente se torne o nosso jogador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ef8c5091995e13270b7e35e83c8d8ab87868120"
   },
   "source": [
    "## Referências\n",
    "- [Intro to Reinforcement Learning - David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n",
    "\n",
    "- [Advanced Deep Learning & Reinforcement Learning](https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)\n",
    "\n",
    "- [CS885 Reinforcement Learning - University of Waterloo](https://www.youtube.com/watch?v=xoxz-OmcL1Q&list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc)\n",
    "\n",
    "\n",
    "## Apêndice\n",
    "\n",
    "A derivada da função *softmax* é a seguinte:\n",
    "\n",
    "Primeiro, note que: $$\\pi_\\theta(s,a) = softmax = \\frac{e^{\\phi(s,a)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\n",
    "\n",
    "Utilizando  $\\log$ identity $\\log(x/y) = \\log(x) - \\log(y)$ podemos escrever $$\\log(\\pi_\\theta(s,a)) = \\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}) $$\n",
    "\n",
    "Agora, tomamos o gradiente:\n",
    "\n",
    "$$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta})$$\n",
    "\n",
    "O lado esquerdo da equação é simplificado como:\n",
    "\n",
    "$$left= \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) = \\nabla_\\theta\\phi(s,a)^\\intercal\\theta = \\phi(s,a)$$\n",
    "\n",
    "O lado direito da equação simplifica para:\n",
    "\n",
    "Utilizando a regra da cadeia: $$\\nabla_x\\log(f(x)) = \\frac{\\nabla_xf(x)}{f(x)}$$\n",
    "\n",
    "Temos:\n",
    "\n",
    "$$right = \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}) = \\frac{\\nabla_\\theta\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\n",
    "\n",
    "Tomando o gradiente do numerador, temos:\n",
    "\n",
    "$$right = \\frac{\\sum_{k=1}^N{\\phi(s,a_k)}e^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\n",
    "\n",
    "Substituindo a definição de $\\pi_\\theta(s,a)$ podemos simplificar para:\n",
    "\n",
    "$$right = \\sum_{k=1}^N{\\phi(s,a_k)}\\pi_\\theta(s,a_k)$$\n",
    "\n",
    "Dada a definição de valor esperado, temos:\n",
    "\n",
    "$$\\mathrm{E}[X] = X \\cdot P = x_1p_1+x_2p_2+ ... +x_np_n$$\n",
    "\n",
    "O que em português traduz para a soma de cada fator vezes sua probabilidade.\n",
    "\n",
    "$$X = features = {\\phi(s,a)}$$\n",
    "\n",
    "$$P = probabilities =\\pi_\\theta(s,a)$$\n",
    "\n",
    "Assim, temos o valor esperado de cada fator:\n",
    "\n",
    "$$right = \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$\n",
    "\n",
    "onde $\\cdot$ significa todas as ações possíveis.\n",
    "\n",
    "Juntando tudo: $$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = left - right = \\phi(s,a) - \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
